<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Kubernetes学习 - cr6588</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="cr6588" /><meta name="description" content="1.minikube(主机非虚拟安装，否则直接跳过) 1.1 安装kubernetctl 1.3 安装virtualbox 1.2 安装minikube [root@cr6588 data]# minikube start Starting local" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.88.1 with theme even" />


<link rel="canonical" href="https://cr6588.github.io/post/docker_k8s/kubernetes%E5%AD%A6%E4%B9%A0/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Kubernetes学习" />
<meta property="og:description" content="1.minikube(主机非虚拟安装，否则直接跳过) 1.1 安装kubernetctl 1.3 安装virtualbox 1.2 安装minikube [root@cr6588 data]# minikube start Starting local" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cr6588.github.io/post/docker_k8s/kubernetes%E5%AD%A6%E4%B9%A0/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2018-09-18T10:20:11+08:00" />
<meta property="article:modified_time" content="2018-09-18T10:20:11+08:00" />

<meta itemprop="name" content="Kubernetes学习">
<meta itemprop="description" content="1.minikube(主机非虚拟安装，否则直接跳过) 1.1 安装kubernetctl 1.3 安装virtualbox 1.2 安装minikube [root@cr6588 data]# minikube start Starting local"><meta itemprop="datePublished" content="2018-09-18T10:20:11+08:00" />
<meta itemprop="dateModified" content="2018-09-18T10:20:11+08:00" />
<meta itemprop="wordCount" content="5593">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubernetes学习"/>
<meta name="twitter:description" content="1.minikube(主机非虚拟安装，否则直接跳过) 1.1 安装kubernetctl 1.3 安装virtualbox 1.2 安装minikube [root@cr6588 data]# minikube start Starting local"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">cr6588</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a><a href="/about">
        <li class="mobile-menu-item">关于</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">cr6588</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about">关于</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Kubernetes学习</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-09-18 </span>
        <div class="post-category">
            <a href="/categories/kubernetes/"> Kubernetes </a>
            </div>
          <span class="more-meta"> 约 5593 字 </span>
          <span class="more-meta"> 预计阅读 12 分钟 </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次阅读 </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h4 id="1minikube主机非虚拟安装否则直接跳过">1.minikube(主机非虚拟安装，否则直接跳过)</h4>
<h5 id="11-安装kubernetctlhttpsgithubcomkuberneteskubernetesblobmasterchangelog-112mdclient-binaries">1.1 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.12.md#client-binaries">安装kubernetctl</a></h5>
<h5 id="13-安装virtualboxhttpswwwcnblogscomwangshuyip6927113html">1.3 <a href="https://www.cnblogs.com/wangshuyi/p/6927113.html">安装virtualbox</a></h5>
<h5 id="12-安装minikubehttpsgithubcomkubernetesminikubereleases">1.2 <a href="https://github.com/kubernetes/minikube/releases">安装minikube</a></h5>
<pre><code>[root@cr6588 data]# minikube start
Starting local Kubernetes v1.10.0 cluster...
Starting VM...
E1113 10:59:55.111232    2586 start.go:168] Error starting host: Error creating host: Error executing step: Running precreate checks.
: We support Virtualbox starting with version 5. Your VirtualBox install is &quot;WARNING: The vboxdrv kernel module is not loaded. Either there is no module\n         available for the current kernel (3.10.0-862.el7.x86_64) or it failed to\n         load. Please recompile the kernel module and install it by\n\n           sudo /sbin/vboxconfig\n\n         You will not be able to start VMs until this problem is fixed.\n5.2.22r126460&quot;. Please upgrade at https://www.virtualbox.org.

/sbin/vboxconfig
yum install gcc  kernel-devel
cd /usr/src/kernels/
#mv 3.10.0-862.14.4.el7.x86_64 3.10.0-862.el7.x86_64

#vboxdrv.sh: Building VirtualBox kernel modules.
#grep: 不匹配的 ) 或 \)

#yum remove kernel-devel
#virtualbox需要本机kernel-devel对应的版本
yum install &quot;kernel-devel-uname-r == $(uname -r)&quot;

#没有开启虚拟化
[root@cr6588 kernels]# minikube start
Starting local Kubernetes v1.10.0 cluster...
Starting VM...
E1113 14:15:48.202789    3254 start.go:168] Error starting host: Error creating host: Error executing step: Running precreate checks.
: This computer doesn't have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory.
</code></pre>
<h4 id="2docker安装18061ce-3el7">2.docker安装（18.06.1.ce-3.el7）</h4>
<pre><code># 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2

# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

#关闭测试版本list（只显示稳定版）
sudo yum-config-manager --enable docker-ce-edge
sudo yum-config-manager --enable docker-ce-test

# 更新yum包索引
yum makecache fast

#NO.1 直接安装Docker CE （will always install the highest  possible version，可能不符合你的需求）
yum install docker-ce

#NO.2 指定版本安装
yum list docker-ce --showduplicates|sort -r
yum install docker-ce-18.06.1.ce-3.el7
#启动并加入服务
systemctl start docker &amp;&amp; systemctl enable docker
#设置私有registry，cgroupdriver,日志
cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;registry-mirrors&quot;: [&quot;https://xxx.com&quot;],
  &quot;insecure-registries&quot; : [&quot;ip:5000&quot;, &quot;xxxxxx.xx.cn:5000&quot;]
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF
#重启生效
systemctl restart docker
</code></pre>
<h4 id="3kubeadm安装1122">3.kubeadm安装（1.12.2）</h4>
<pre><code>#修改主机名称
hostnamectl set-hostname   localhost.localdomain name
#关闭swap
swapoff -a
vi /etc/fstab
注释swap

systemctl stop firewalld
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
加载ipvs相关模块以及安装依赖关系
yum install ipset ipvsadm conntrack-tools.x86_64 -y

modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh
modprobe ip_vs
</code></pre>
<blockquote>
<pre><code>重启会失效，参考[Linux启动自动加载模块](https://www.jianshu.com/p/69e0430a7d20)在/etc/sysconfig/modules/建立ipvs.modules并写入
#!/bin/bash
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh
modprobe ip_vs
给ipvs.modules加入执行权限
chmod +x ipvs.modules
</code></pre>
</blockquote>
<pre><code>查看是否成功
lsmod| grep ip_vs
2、开启内核转发，并使之生效，末尾EOF前不要有空格
cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl -p /etc/sysctl.d/k8s.conf

cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
    http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install kubelet-1.12.2-0 kubeadm-1.12.2-0 kubectl-1.12.2-0
#yum install kubelet kubeadm kubectl
systemctl enable kubelet &amp;&amp; systemctl start kubelet
# Run kubeadm config images pull prior to kubeadm init to verify connectivity to gcr.io registries.验证是否能拉取相关镜像，若不能则找到相关镜像库镜像拉取
</code></pre>
<blockquote>
<p>1.17可以增加image-repository参数，修改默认镜像来源，不再从私有库，直接从<a href="registry.cn-hangzhou.aliyuncs.com/google_containers">阿里镜像库</a>拉取
kubeadm init &ndash;image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers</p>
</blockquote>
<pre><code>kubeadm init xxx --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers
#master节点执行，network使用flannel前置条件.若需要制定版本则加入--kubernetes-version=v1.12.2
kubeadm init --pod-network-cidr=10.244.0.0/16
#master节点执行，network使用calico前置条件.若需要制定版本则加入
kubeadm init --pod-network-cidr=192.168.0.0/16
#master节点执行，network使用weave net前置条件
kubeadm init
#出错时还原
kubeadm reset
#记录下从节点加入命令
kubeadm join xxx:6443 --token gd7nxxxxxx --discovery-token-ca-cert-hash sha256:xxxx
#To make kubectl work for your non-root user, run these commands, which are also part of the kubeadm init output:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
# Alternatively, if you are the root user, you can run:
export KUBECONFIG=/etc/kubernetes/admin.conf

#使用flannel（不支持NetworkPolicy,部署的应用会无视firewalld完全暴露在公网中）
sysctl net.bridge.bridge-nf-call-iptables=1
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
#使用calico,当含有master,node节点时不能正常启动，参照提示的错误实例解决
kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
</code></pre>
<p>kubectl apply -f <a href="https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml">https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml</a>
#使用weave net（支持NetworkPolicy,但重启防火墙时会失效，需要
systemctl daemon-reload
systemctl restart kubelet
systemctl restart docker生效）
kubectl apply -f &ldquo;<a href="https://cloud.weave.works/k8s/net?k8s-version=$(kubectl">https://cloud.weave.works/k8s/net?k8s-version=$(kubectl</a> version | base64 | tr -d &lsquo;\n&rsquo;)&rdquo;
#查看状态
kubectl get pods &ndash;all-namespaces</p>
<blockquote>
<pre><code>...failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.1.1/24
删除cni0网卡（ip link delete cni0）可以解决，但为了预防以后莫名错误，完全重置一次，参见https://github.com/kubernetes/kubernetes/issues/39557
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
ifconfig docker0 up
systemctl start docker
systemctl start kubelet
</code></pre>
</blockquote>
<blockquote>
<pre><code>...Readiness probe failed: calico/node is not ready: BIRD is not ready: BGP not established with 10.192.0.1
calico健康检查出错，calico的ip自动检测给了一个错误IP10.192.0.1，在将https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml下载下来，在calico-node的env加入IP_AUTODETECTION_METHOD,其eth.*视其节点间通信的网卡决定
</code></pre>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">...
            - name: IP
              value: &#34;autodetect&#34;
            - name: IP_AUTODETECTION_METHOD
              value: &#34;interface=eth.*&#34;
...
</code></pre></td></tr></table>
</div>
</div><blockquote>
<pre><code>calico容器无法ping通域名，多方查找未果。参照[calico网络模型中的路由原理](https://segmentfault.com/a/1190000016565044)禁用ipip，找到CALICO_IPV4POOL_IPIP将其value设置成Off或者never解决。之后再其官网找到关于此字段说明[environment-variables](https://docs.projectcalico.org/v3.5/reference/node/configuration#environment-variables)
</code></pre>
</blockquote>
<pre><code>#主节点向指定ip开放相关端口
firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;www.xxxx.cn&quot; port protocol=&quot;tcp&quot; port=&quot;6443&quot; accept&quot;
firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;www.xxxx.cn&quot; port protocol=&quot;tcp&quot; port=&quot;2379-2380&quot; accept&quot;
firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;www.xxxx.cn&quot; port protocol=&quot;tcp&quot; port=&quot;10250-10252&quot; accept&quot;
#从节点向指定ip开放相关端口
firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;www.xxxx.cn&quot; port protocol=&quot;tcp&quot; port=&quot;10250&quot; accept&quot;
firewall-cmd --permanent --add-rich-rule=&quot;rule family=&quot;ipv4&quot; source address=&quot;www.xxxx.cn&quot; port protocol=&quot;tcp&quot; port=&quot;30000-32767&quot; accept&quot;
#默认主节点不会编排容器，若希望启用则运行
kubectl taint nodes --all node-role.kubernetes.io/master-
#从节点加入主节点
#修改节点名称
hostnamectl --static set-hostname [主机名]
#在node的hosts文件加入解析
127.0.0.1 localhost [主机名]
#使用刚刚的kubeadm join
kubeadm join xxx:6443 --token gd7nxxxxxx --discovery-token-ca-cert-hash sha256:xxxx
#token有效期24小时，过期之后用创建token，查找hash值即可
kubeadm token create
kubeadm token list
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
#主节点查看节点信息
kubectl get nodes
</code></pre>
<blockquote>
<p>init之后如果关机重启kubelet正常的话，root用户若使用kubelctl提示无法连接则
export KUBECONFIG=/etc/kubernetes/admin.conf</p>
</blockquote>
<h4 id="4dashboard安装">4.dashboard安装</h4>
<pre><code>#由于执行过一次最简安装后无法访问本地，查看kubectl get pods --all-namespaces
kube-system   kube-scheduler-localhost.localdomain            1/1     Running             0          91m
kube-system   kubernetes-dashboard-77fd78f978-4vzk2           0/1     ContainerCreating   0          13m
一直处于ContainerCreating中，于是删除,又通过自检证书方式重建
kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
#创建自已的证书
openssl req \
-newkey rsa:4096 -nodes -sha256 -keyout certs/dashboard.key \
-x509 -days 3650 -out certs/dashboard.crt

kubectl create secret generic kubernetes-dashboard-certs --from-file=$HOME/certs -n kube-system

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
#查看服务，发现服务运行但无法访问
kubectl get service kubernetes-dashboard -n kube-system
#删除
kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
#下载文件
curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
#在kubernetes-dashboard.yaml中添加service的type为NodePort
...
  namespace: kube-system
spec:
    type: NodePort
    ports:
        - port: 443
...
#重新生成密钥并部署
kubectl create secret generic kubernetes-dashboard-certs --from-file=$HOME/certs -n kube-system
kubectl apply -f kubernetes-dashboard.yaml
#查看服务映射的本地端口
kubectl get service kubernetes-dashboard -n kube-system
NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.109.126.132   &lt;none&gt;        443:31474/TCP   20m
#访问子节点https://ip:31474, 需要登录。若没有登录界面，直接进入，访问https://192.168.199.206:31474/#!/login
![x](/images/k8s_login.png)
#创建用户
cat &lt;&lt;EOF &gt; dashboard-adminuser.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

kubectl apply -f dashboard-adminuser.yaml
#查找，复制admin-user的token，登录即可
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')

#查看日志
kubectl describe pod kubernetes-dashboard-77fd78f978-4vzk2无法查看描述必须加上--namespace=kube-system
kubectl describe pod kubernetes-dashboard-77fd78f978-4vzk2 --namespace=kube-system
kubectl logs kubernetes-dashboard-77fd78f978-4vzk2 -n kube-system

...failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.244.1.1/24

kubectl get pods --all-namespaces
kubectl get service kubernetes-dashboard -n kube-system
</code></pre>
<blockquote>
<p>使用flannel启用防火墙后dashboard可能无法安装，需要开启
firewall-cmd &ndash;permanent &ndash;add-masquerade # 允许防火墙伪装
并开启相关端口，参照<a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">install-kubeadm</a>开启
使用calico必须启用防火墙，需要开启
firewall-cmd &ndash;permanent &ndash;add-masquerade # 允许防火墙伪装
并开启相关端口，参照<a href="https://docs.projectcalico.org/v3.3/getting-started/kubernetes/requirements">System requirements</a>开启
5473需要开启否则应用内部无法通过服务名:端口访问
使用weave-net必须启用防火墙，需要开启
firewall-cmd &ndash;permanent &ndash;add-masquerade # 允许防火墙伪装
并开启相关端口，参照<a href="https://www.weave.works/docs/net/latest/faq/">FAQ</a>开启
TCP 6783-6784 and UDP 6783-6784需要开启否则应用内部通过服务名:端口访问会时快时慢</p>
</blockquote>
<h4 id="redis安装">redis安装</h4>
<pre><code>参见docker hub中[redis](https://hub.docker.com/_/redis/)官方镜像文档说明，直接在dashboard上进行图形化安装
</code></pre>
<blockquote>
<p>安装后查看服务的yaml文档中的端口发现
&ldquo;protocol&rdquo;: &ldquo;TCP&rdquo;,
&ldquo;port&rdquo;: 6388,
&ldquo;targetPort&rdquo;: 6379,
&ldquo;nodePort&rdquo;: 31783
其中port是集群内部的端口，31783是节点以及外部可以访问的端口，6388是pod的端口，也就是若外网链接访问时首先经过31783进入然后经过6388最后指向6379。使用kubectl expose deployment/redis &ndash;type=&ldquo;NodePort&rdquo; &ndash;port 6379创建service时，port与targetPort都会是6379</p>
</blockquote>
<p>图形化安装之后删除,创建redis持久化安装</p>
<pre><code>#创建pv与pvc
vi redis-pv.yaml
</code></pre>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kind: PersistentVolume
apiVersion: v1
metadata:
  name: mysql-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &#34;/mnt/data&#34;
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
</code></pre></td></tr></table>
</div>
</div><pre><code>kubectl create -f redis-pv.yaml
#会在从节点生成/data/redis-data目录

#创建deployment
{
  &quot;kind&quot;: &quot;Deployment&quot;,
  &quot;apiVersion&quot;: &quot;extensions/v1beta1&quot;,
  &quot;metadata&quot;: {
	&quot;name&quot;: &quot;redis-persistent&quot;,
	&quot;namespace&quot;: &quot;default&quot;,
	&quot;labels&quot;: {
      &quot;k8s-app&quot;: &quot;redis-persistent&quot;
	}
  },
  &quot;spec&quot;: {
	&quot;replicas&quot;: 1,
	&quot;selector&quot;: {
      &quot;matchLabels&quot;: {
        &quot;k8s-app&quot;: &quot;redis-persistent&quot;
      }
	},
	&quot;template&quot;: {
      &quot;metadata&quot;: {
        &quot;name&quot;: &quot;redis-persistent&quot;,
        &quot;labels&quot;: {
          &quot;k8s-app&quot;: &quot;redis-persistent&quot;
        }
      },
      &quot;spec&quot;: {
        &quot;containers&quot;: [
          {
            &quot;name&quot;: &quot;redis-persistent&quot;,
            &quot;image&quot;: &quot;redis:5&quot;,
            &quot;command&quot;: [
              &quot;redis-server&quot;
            ],
            &quot;args&quot;: [
              &quot;--appendonly yes&quot;
            ],
            &quot;volumeMounts&quot;: [
              {
                &quot;name&quot;: &quot;redis-persistent-storage&quot;,
                &quot;mountPath&quot;: &quot;/data&quot;
              }
            ]
          }
        ],
        &quot;volumes&quot;: [
          {
            &quot;name&quot;: &quot;redis-persistent-storage&quot;,
            &quot;persistentVolumeClaim&quot;: {
              &quot;claimName&quot;: &quot;redis-pv-claim&quot;
            }
          }
        ]
      }
	}
  }
}
#当要使用自己的redis.conf时将自己的conf文件复制到从节点/data/redis-data下，args增加/data/xxx.conf即可
</code></pre>
<blockquote>
<p>挂载磁盘时注意路径，不要将磁盘挂载到镜像本身就有的路径，例如/data中本身含有应用相关文件，然后又挂载到/data时，会覆盖镜像已有的data</p>
</blockquote>
<h4 id="部署自己的应用">部署自己的应用</h4>
<p>在创建好自己的镜像并上传上去后，通过dashboard安装时发现镜像始终无法拉取。但是通过docker直接拉取私有镜像时可以的。搜寻一番之后发现是k8s拉取私有镜像时需要docker相关账号信息。</p>
<h5 id="配置secret拉取私有仓库镜像">配置secret拉取私有仓库镜像</h5>
<p>kubectl create secret docker-registry registrysecret &ndash;docker-server=ip或域名:端口号  &ndash;docker-username=admin &ndash;docker-password=xxxx
有2种方式加载secret</p>
<h6 id="1将密钥加载到yaml文件">1.将密钥加载到yaml文件</h6>
<pre><code>apiVersion: v1
kind: ReplicationController
metadata:
  name: webapp
spec:
  replicas: 2
  template:
	metadata:
      name: webapp
      labels:
        app: webapp
	spec:
      containers:
      - name: webapp
        imagePullPolicy: Always
        image: e5:8889/tomcat:latest
        ports:
          - containerPort: 80
      imagePullSecrets:
      - name: registrysecret
</code></pre>
<h6 id="2将secret直接加载到默认帐号中">2.将secret直接加载到默认帐号中</h6>
<pre><code>kubectl patch serviceaccount default -p '{&quot;imagePullSecrets&quot;: [{&quot;name&quot;: &quot;registrysecret&quot;}]}'
#查看帐号配置
kubectl get serviceaccounts default -o yaml
#在dashboard更改下版本或者删除重建部署即可
</code></pre>
<h5 id="启动应用后pod中容器不能通过域名访问外网">启动应用后pod中容器不能通过域名访问外网</h5>
<p>#进入容器查看
kubectl get pods
kubectl kubectl exec -it pod名称 &ndash; /bin/bash
#退出
crtl+p &amp;&amp; crtl+q</p>
<blockquote>
<p>如果Pod具有多个Container，请使用&ndash;container或-c在kubectl exec命令中指定Container 。例如，假设您有一个名为my-pod的Pod，而Pod有两个名为main-app和helper-app的容器。以下命令将打开主应用程序Container的shell。</p>
</blockquote>
<pre><code>#交互模式进入容器
kubectl exec -it my-pod --container main-app -- /bin/bash
</code></pre>
<p>网上搜索一番发现在https://github.com/kubernetes/kubernetes/issues/57096#issuecomment-351029125中看到是防火墙未开启udp端口的问题，但具体是哪一个端口不能确定，经过同事之前更改dns服务器之后就能在容器中ping通，应该是dns问题，在容器中更改/etc/resolv.conf之后发现可以ping通，但不能每次容器都要去更改吧，也不应该这样做，所以又寻找其它方法，又搜索了很久发现很多人都遇到过这个问题，一部分是通过sysctl net.bridge.bridge-nf-call-iptables=1可以恢复，但测试之后未能成功。在搜索过程看到一些处理这个问题的过程是弄清ping之后请求的走向，通过查看防火墙日志确定在哪一过程被拦截，但这方面积累比较薄弱，且dns的容器无法进入，于是又继续查找。之后把防火墙关了重启服务器发现可以ping通,kubectl exec my-pod ping <a href="http://www.baidu.com">www.baidu.com</a>,认为是防火墙端口的问题，而dns是coredns是通过安装flanel带来的，最后去flanel的<a href="https://github.com/coreos/flannel/blob/master/Documentation/troubleshooting.md#firewalls">Troubleshooting</a>的中firewalls指出</p>
<pre><code>When using udp backend, flannel uses UDP port 8285 for sending encapsulated packets.
When using vxlan backend, kernel uses UDP port 8472 for sending encapsulated packets.
</code></pre>
<p>但不能确定是哪一个就一个个测试，最终发现在节点启用8472/udp后，在master执行</p>
<pre><code>systemctl daemon-reload
systemctl restart kubelet
systemctl restart docker
</code></pre>
<p>master再kubectl exec my-pod ping <a href="http://www.baidu.com">www.baidu.com</a>就能ping通，但速度较慢，在master也启用8472/udp后ping的速度明显加快</p>
<blockquote>
<p>回顾整个过程可以发现在最初是找对方向的，但之后又跑偏了，说明对k8s的网络管理方面理解的很薄弱，然后对centos7如何监控请求走向还是不会，待提高的地方很多。</p>
</blockquote>
<h5 id="容器需要使用host">容器需要使用host</h5>
<p>在只有docker时可以通过docker run时加入&ndash;add-host=&ldquo;localhost example.com&rdquo;:127.0.0.1，而在k8s时可以在pod的<a href="https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/">spec.hostAliases</a>实现。eg:</p>
<pre><code>...
    spec:
  hostAliases:
  - ip: &quot;10.101.4.13&quot;
    hostnames:
    - &quot;xxx.xx.xx&quot;
  - ip: &quot;xxx.xx.xx.xxx&quot;
    hostnames:
    - &quot;user.xx.xxx.cn&quot;
  - ip: &quot;118.xxx.xx.xxx&quot;
    hostnames:
    - &quot;picture.xxx.cn&quot;
    - &quot;static.xxx.cn&quot;
  containers:
  - name: ...
  ...
</code></pre>
<h5 id="statefulset应用">statefulset应用</h5>
<p>有状态应用于headless service搭配，它的主要特点是ClusterIP为None,以一个zookeeper应用为例</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kind: PersistentVolume
apiVersion: v1
metadata:
  name: zookeeper-pv-0
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: &#34;/docker/pv/zookeeper&#34;
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  labels:
    app: zookeeper
spec:
  ports:
  - port: 2181
    name: zookeeper
  clusterIP: None
  selector:
    app: zookeeper
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
spec:
  serviceName: &#34;zookeeper&#34;
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: zookeeper
        ports:
        - containerPort: 2181
          name: zookeeper
        volumeMounts:
        - name: zk
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: zk
    spec:
      accessModes: [ &#34;ReadWriteOnce&#34; ]
      resources:
        requests:
          storage: 1Gi
</code></pre></td></tr></table>
</div>
</div><p>由于未配置动态存储，先设置一个hostPath类型的pv，然后设置headless service,最后设置StatefulSet，其中volumeClaimTemplates中描述了pvc，会查询符合要求的pv后自动创建pvc,在StatefulSet中pod与pvc会绑定，当伸缩数量为2时，需要新建pv满足</p>
<h4 id="参考文献">参考文献</h4>
<ol>
<li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">https://kubernetes.io/docs/setup/independent/install-kubeadm/</a></li>
<li><a href="https://www.cnblogs.com/crysmile/p/9648406.html">https://www.cnblogs.com/crysmile/p/9648406.html</a></li>
<li><a href="http://blog.51cto.com/11887934/2050590">http://blog.51cto.com/11887934/2050590</a></li>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/</a></li>
</ol>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">cr6588</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2018-09-18
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      
      <nav class="post-nav">
        <a class="prev" href="/post/docker_k8s/docker%E4%B8%8Ek8s%E7%AE%80%E4%BB%8B/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Docker与k8s简介</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/docker_k8s/kubernetes%E5%AE%9E%E9%AA%8C/">
            <span class="next-text nav-default">Kubernetes实验</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" crossorigin="anonymous"></script>
    <script type="text/javascript">
      var gitalk = new Gitalk({
        id: '2018-09-18 10:20:11 \u002b0800 CST',
        title: 'Kubernetes学习',
        clientID: 'f8fc0224f2d6ffa6eba8',
        clientSecret: 'e1824b481f1127d0d906798b415d2825beeec064',
        repo: 'cr6588.github.io',
        owner: 'cr6588',
        admin: ['cr6588'],
        body: decodeURI(location.href)
      });
      gitalk.render('gitalk-container');
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/gitalk/gitalk">comments powered by gitalk.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:cr6588@vip.qq.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/cr6588" class="iconfont icon-github" title="github"></a>
  <a href="https://cr6588.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> 本站总访问量 <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 次 </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> 本站总访客数 <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> 人 </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2019 - 
    2022<span class="heart"><i class="iconfont icon-heart"></i></span><span>cr6588</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>








</body>
</html>
